{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-step time series forecasting\n",
    "\n",
    "A multi-step time series forecasting model is built in this notbook. In other word, past sequences are used to forecast the next 90 steps in the future. This fits in the the idea of seq2seq model, in which both inputs and outputs are sequences. For this task, the dataset needs to be prepared accordingly. \n",
    "\n",
    "Since we have 10 * 50 independent time series, I find it's easy to use spark window functions to handle feature and sequence generation. And then convert the Spark DataFrame to a PyTorch DataLoader using petastorm spark_dataset_converter. More information can be found in the following link:\n",
    "* https://databricks.com/notebooks/simple-aws/petastorm-spark-converter-pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip -q install pyspark\n",
    "!pip -q install petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "from petastorm import TransformSpec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def smape(forecast, actual):\n",
    "    f = np.asarray(forecast)\n",
    "    a = np.asarray(actual)\n",
    "    up = np.abs(f - a)\n",
    "    down = (np.abs(f) + np.abs(a))/2\n",
    "    np.mean(up/down)\n",
    "    return np.mean(up/down)*100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "n_store = 10\n",
    "n_item = 50\n",
    "n_ts = 1826\n",
    "n_pred = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"retail_demand_forecasting\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- item: integer (nullable = true)\n",
      " |-- sales: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- item: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"date\", DateType()),StructField(\"store\", IntegerType()),\n",
    "                     StructField(\"item\", IntegerType()),StructField(\"sales\", FloatType())])\n",
    "train_df = spark.read.csv(path = '/kaggle/input/demand-forecasting-kernels-only/train.csv', schema=schema, header = True).cache()\n",
    "train_df.printSchema()\n",
    "\n",
    "schema = StructType([StructField(\"id\", IntegerType()),\n",
    "                     StructField(\"date\", DateType()),StructField(\"store\", IntegerType()),\n",
    "                     StructField(\"item\", IntegerType())])\n",
    "test_df = spark.read.csv(path = '/kaggle/input/demand-forecasting-kernels-only/test.csv', schema=schema, header = True).cache()\n",
    "test_df.printSchema()\n",
    "\n",
    "\n",
    "train_df = train_df.withColumn('type',f.lit(\"train\"))\n",
    "train_df = train_df.withColumn('id',f.lit(None))\n",
    "\n",
    "test_df = test_df.withColumn('type',f.lit(\"test\"))\n",
    "test_df = test_df.withColumn('sales',f.lit(None))\n",
    "\n",
    "df = train_df.unionByName(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. make features use spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logTransform(Transformer):\n",
    "    def __init__(self, inputCol, outputCol):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, df):\n",
    "        return df.withColumn(self.outputCol, f.log1p(f.col(self.inputCol)))\n",
    "    \n",
    "class targetMaker(Transformer):\n",
    "    def __init__(self, inputCol, outputCol='target', dateCol='date', idCol=['store', 'item'], Range = 90):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.dateCol = dateCol\n",
    "        self.idCol = idCol\n",
    "        self.Range = Range\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        w = Window.partitionBy(self.idCol).orderBy(self.dateCol).rowsBetween(0, self.Range - 1)\n",
    "        df = df.withColumn(self.outputCol, f.collect_list(self.inputCol).over(w))\n",
    "        return df\n",
    "    \n",
    "class seriesMaker(Transformer):\n",
    "    def __init__(self, inputCol, outputCol='input', dateCol='date', idCol=['store', 'item'], Range = 120):\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.dateCol = dateCol\n",
    "        self.idCol = idCol\n",
    "        self.Range = Range\n",
    "        \n",
    "    def _transform(self, df):\n",
    "        w = Window.partitionBy(self.idCol).orderBy(self.dateCol).rowsBetween(-self.Range, -1)\n",
    "        df = df.withColumn(self.outputCol, f.collect_list(self.inputCol).over(w))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- item: integer (nullable = true)\n",
      " |-- sales: float (nullable = true)\n",
      " |-- type: string (nullable = false)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- logSales: double (nullable = true)\n",
      " |-- target: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- input: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- storeVec: vector (nullable = true)\n",
      " |-- itemVec: vector (nullable = true)\n",
      " |-- covariates: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 180\n",
    "# Feature extraction\n",
    "logt = logTransform(inputCol ='sales', outputCol='logSales')\n",
    "tgtm = targetMaker(inputCol = 'logSales', Range = n_pred)\n",
    "srsm = seriesMaker(inputCol = 'logSales', Range = WINDOW_SIZE)\n",
    "encoder = OneHotEncoder(inputCols=[\"store\",\"item\",],outputCols=[\"storeVec\",\"itemVec\"])\n",
    "assembler = VectorAssembler(inputCols=[\"storeVec\",\"itemVec\"], outputCol=\"covariates\")\n",
    "\n",
    "pipeline = Pipeline(stages=[logt, tgtm, srsm, encoder, assembler])\n",
    "processing = pipeline.fit(df)\n",
    "transformed = processing.transform(df)\n",
    "transformed.printSchema()\n",
    "\n",
    "transformed_train = transformed.filter( (f.size('input')>=WINDOW_SIZE) & (f.size('target')>=n_pred) & (f.col('type') == 'train') )\n",
    "transformed_test = transformed.filter( (f.size('input')>=WINDOW_SIZE) & (f.col('type') == 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cache the Spark DataFrame using Petastorm Spark converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 778500, val: 500\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:///dbfs/tmp/petastorm/cache\")\n",
    "\n",
    "converter_train = make_spark_converter(transformed_train.select('input','covariates','target'))\n",
    "converter_test = make_spark_converter(transformed_test.select('input','covariates', 'store', 'item'))\n",
    "\n",
    "print(f\"train: {len(converter_train)}, val: {len(converter_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiStepModel(nn.Module):\n",
    "    def __init__(self,  input_size = 1, covar_size = 60, output_size = 1, output_seq_len = 90, h1_dim = 32, h2_dim = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.h1_dim = h1_dim\n",
    "        self.h2_dim = h2_dim\n",
    "        \n",
    "        self.lstm_layer1 = nn.LSTM(input_size + covar_size, h1_dim, num_layers = 1, batch_first = True)\n",
    "        self.lstm_layer2 = nn.LSTM(h1_dim + covar_size, h2_dim, num_layers = 1, batch_first = True)\n",
    "        self.fc_layer = nn.Linear(h2_dim, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "\n",
    "    def forward(self, input, covar):\n",
    "        input_seq_len = input.size(1)\n",
    "        x = covar.unsqueeze(1).repeat(1, input_seq_len, 1) # expand to input seq length\n",
    "        lstm1_input = torch.cat([input, x], dim = 2) # combine with input seq\n",
    "        output, (hn, cn) = self.lstm_layer1(lstm1_input)\n",
    "        lstm1_output = F.relu(hn[-1]) # get the last hidden state of the last LSTM layer\n",
    "        \n",
    "        x = torch.cat([lstm1_output, covar], dim = 1).unsqueeze(1) # combine with covariates\n",
    "        lstm2_input = x.repeat(1, self.output_seq_len, 1)\n",
    "        output, (hn, cn) = self.lstm_layer2(lstm2_input)\n",
    "        lstm2_output = F.relu(output)\n",
    "        \n",
    "        fc_input = self.dropout(lstm2_output)\n",
    "        out = self.fc_layer(lstm2_output).squeeze()\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataiter, steps_per_epoch):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    curr_loss = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        pd_batch = next(dataiter)\n",
    "        x1 = pd_batch['input'].unsqueeze(2).to(device)\n",
    "        x2 = pd_batch['covariates'].to(device)\n",
    "        y = pd_batch['target'].to(device)\n",
    "    \n",
    "        # Track history in training\n",
    "        with torch.set_grad_enabled(True):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(x1, x2)\n",
    "            loss = loss_fn(out, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        curr_loss.append(loss.item())\n",
    "        print('\\rprogress {:6.2f} %\\tloss {:8.4f}'.format(round(100*step/steps_per_epoch, 2), np.mean(curr_loss)), end = \"\")\n",
    "  \n",
    "    epoch_loss = np.mean(curr_loss)\n",
    "    print('\\rprogress {:6.2f} %\\tloss {:8.4f}'.format(round(100*step/steps_per_epoch, 2), epoch_loss ))\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 1/50\t6082 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n",
      "  column_as_pandas = column.data.chunks[0].to_pandas()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 100.00 %\tloss   0.3382\n",
      "2m 0s\n",
      "----------\n",
      "Epoch 2/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0792\n",
      "4m 1s\n",
      "----------\n",
      "Epoch 3/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0751\n",
      "6m 1s\n",
      "----------\n",
      "Epoch 4/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0698\n",
      "8m 1s\n",
      "----------\n",
      "Epoch 5/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0641\n",
      "10m 1s\n",
      "----------\n",
      "Epoch 6/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0569\n",
      "12m 0s\n",
      "----------\n",
      "Epoch 7/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0540\n",
      "14m 0s\n",
      "----------\n",
      "Epoch 8/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0517\n",
      "15m 59s\n",
      "----------\n",
      "Epoch 9/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0502\n",
      "17m 58s\n",
      "----------\n",
      "Epoch 10/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0485\n",
      "19m 59s\n",
      "----------\n",
      "Epoch 11/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0476\n",
      "22m 0s\n",
      "----------\n",
      "Epoch 12/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0470\n",
      "24m 1s\n",
      "----------\n",
      "Epoch 13/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0464\n",
      "26m 3s\n",
      "----------\n",
      "Epoch 14/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0463\n",
      "28m 2s\n",
      "----------\n",
      "Epoch 15/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0459\n",
      "30m 2s\n",
      "----------\n",
      "Epoch 16/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0456\n",
      "32m 0s\n",
      "----------\n",
      "Epoch 17/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0454\n",
      "33m 59s\n",
      "----------\n",
      "Epoch 18/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0452\n",
      "35m 58s\n",
      "----------\n",
      "Epoch 19/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0451\n",
      "37m 56s\n",
      "----------\n",
      "Epoch 20/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0451\n",
      "39m 55s\n",
      "----------\n",
      "Epoch 21/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0448\n",
      "41m 55s\n",
      "----------\n",
      "Epoch 22/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0449\n",
      "43m 54s\n",
      "----------\n",
      "Epoch 23/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0447\n",
      "45m 53s\n",
      "----------\n",
      "Epoch 24/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0321\n",
      "67m 43s\n",
      "----------\n",
      "Epoch 35/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0326\n",
      "69m 41s\n",
      "----------\n",
      "Epoch 36/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0320\n",
      "83m 32s\n",
      "----------\n",
      "Epoch 43/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0310\n",
      "91m 29s\n",
      "----------\n",
      "Epoch 47/50\t6082 batches\n",
      "progress 100.00 %\tloss   0.0308\n",
      "99m 40s\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "model = multiStepModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "start = time.time()\n",
    "with converter_train.make_torch_dataloader(batch_size = BATCH_SIZE) as trainloader:\n",
    "    trainiter = iter(trainloader)\n",
    "    steps_per_epoch = len(converter_train) // BATCH_SIZE\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}\\t{} batches'.format(epoch, NUM_EPOCHS, steps_per_epoch))\n",
    "        epoch_loss = train_one_epoch(trainiter, steps_per_epoch)\n",
    "        print('{}'.format(timeSince(start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = test_df.select('date').distinct().sort('date').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, converter_test):\n",
    "    \n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    \n",
    "    with converter_test.make_torch_dataloader(batch_size = 1, num_epochs = 1) as testloader:\n",
    "        testiter = iter(testloader)\n",
    "        steps_per_epoch = len(converter_test)\n",
    "\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "        for step in range(1, steps_per_epoch+1):\n",
    "        \n",
    "            pd_batch = next(testiter)\n",
    "            x1 = pd_batch['input'].unsqueeze(2).to(device)\n",
    "            x2 = pd_batch['covariates'].to(device)\n",
    "    \n",
    "            with torch.set_grad_enabled(False):\n",
    "                out = model(x1, x2)\n",
    "        \n",
    "            curr_df = date_df\n",
    "            curr_df['sales'] = np.expm1(out.tolist()).round()\n",
    "            curr_df['store'] = pd_batch['store'].item()\n",
    "            curr_df['item'] = pd_batch['item'].item()\n",
    "            final_df = final_df.append(curr_df)\n",
    "        \n",
    "            print('\\rprogress {:6.2f} %'.format(round(100*step/steps_per_epoch, 2)), end = \"\")\n",
    "        print('\\rprogress {:6.2f} %'.format(round(100*step/steps_per_epoch, 2)))\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 100.00 %\n"
     ]
    }
   ],
   "source": [
    "final_df = forecast(model, converter_test)\n",
    "\n",
    "test_df = test_df.drop('sales')\n",
    "sub_df = test_df.toPandas().merge(final_df, how = 'left', on = ['date','store','item'])\n",
    "\n",
    "sub_df[['id','sales']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
